{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96ec1e48",
   "metadata": {},
   "source": [
    "Environment: pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f5c971",
   "metadata": {},
   "source": [
    "# <font color='purple'>Convolutional Neural Network\n",
    "In this notebook we train a convolutional neural network to classify images into one of the 10 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "538c2eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import optuna\n",
    "from optuna.trial import TrialState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef8a321f",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7571f6c3",
   "metadata": {},
   "source": [
    "## <font color = 'blue'>Import Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38f1c936",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_mean = torch.load('mean.pt')\n",
    "my_std = torch.load('std_dev.pt')\n",
    "\n",
    "def get_CIFAR(train):\n",
    "    \"\"\"\n",
    "    Function to download and transform CIFAR dataset\n",
    "    :param train: Boolean value. If True, return training dataset. If False return test dataset.\n",
    "    :param mean: sequence of means for each channel, to be used for normalisation\n",
    "    :param std_dev: sequence of std deviations for each channel, to be used for normalisation\n",
    "    :return: Dataset\n",
    "    \"\"\"\n",
    "    my_transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                       transforms.Normalize(my_mean, my_std)])\n",
    "\n",
    "    my_cifar = torchvision.datasets.CIFAR10(root='./data', train=train, download=True, transform=my_transform)\n",
    "\n",
    "    return my_cifar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bad6407c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "trainset = get_CIFAR(train = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800eff02",
   "metadata": {},
   "source": [
    "## <font color = 'blue'>Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c7a905",
   "metadata": {},
   "source": [
    "**Tune hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "303617a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_parameters(trial, my_optimizer):\n",
    "    \"\"\"\n",
    "    Set parameters for neural network, optimisation algorithm etc.\n",
    "    :param trial: Optuna trial object\n",
    "    :param my_optimizer: optimizer to use - SGD / SGD_classical / SGD_nesterov / Adam\n",
    "\n",
    "    :return: dictionary of parameters:\n",
    "            - n_conv_layers: number of convolution layers in neural network\n",
    "            - out_ch_conv{i}: number of output channels in convolution layer i\n",
    "            - kernel_conv{i}_even: kernel width in convolution layer i - even option\n",
    "            - kernel_conv{i}_odd:                                      - odd option\n",
    "\n",
    "            - n_linear_layers: number of linear layers in neural network\n",
    "            - n_units_lin{i}: number of units in linear layer i\n",
    "            - dropout_lin{i}: dropout probability for linear layer i\n",
    "\n",
    "            - lr: learning rate\n",
    "            - batch_size: batch size\n",
    "            - n_epochs = number of epochs (i.e. number of passes through training data during optimisation)\n",
    "    \"\"\"\n",
    "    trial.suggest_int(\"n_conv_layers\", 2, 2)\n",
    "\n",
    "    for i in range(trial.params['n_conv_layers']):\n",
    "        trial.suggest_int(f'out_ch_conv{i}', 1, 50)\n",
    "        trial.suggest_categorical(f'kernel_conv{i}_even', [2, 4, 6])\n",
    "        trial.suggest_categorical(f'kernel_conv{i}_odd', [3, 5, 7])\n",
    "\n",
    "    trial.suggest_int(\"n_linear_layers\", 1, 3)\n",
    "\n",
    "    for i in range(trial.params['n_linear_layers']):\n",
    "        trial.suggest_int(f'n_units_lin{i}', 1, 200)\n",
    "        trial.suggest_float(f\"dropout_lin{i}\", 0.1, 0.9)\n",
    "\n",
    "    trial.suggest_float(\"lr\", 1e-5, 1e-1, log=True)\n",
    "    trial.suggest_int(\"batch_size\", 10, 10)\n",
    "    trial.suggest_int(\"n_epochs\", 5, 5)\n",
    "\n",
    "    trial.suggest_categorical(\"optimizer\", [my_optimizer])\n",
    "    if (my_optimizer=='SGD_classical') | (my_optimizer=='SGD_nesterov'):\n",
    "        trial.suggest_float(\"momentum\", 0.6, 0.999)\n",
    "    elif my_optimizer=='Adam':\n",
    "        trial.suggest_float(\"beta1\", 0.6, 0.999)\n",
    "        trial.suggest_float(\"beta2\", 0.8, 0.999)\n",
    "\n",
    "    my_params = trial.params\n",
    "\n",
    "    return my_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "728219c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(my_params):\n",
    "    \"\"\"Defines convolutional neural network based on set parameters\n",
    "    :param my_params: dictionary of parameters (see set_parameters() for full list)\n",
    "    \"\"\"\n",
    "\n",
    "    layers = []\n",
    "\n",
    "    # Define Convolution Layers\n",
    "    in_ch = 3  # number of input channels = no. of channels in feature matrix = 3 (RGB)\n",
    "    img_width = 32 # number of px along length & width of feature matrix\n",
    "    for i in range(my_params['n_conv_layers']):\n",
    "        # convolution layer\n",
    "        out_ch = my_params[f'out_ch_conv{i}']  # number of output channels for this layer\n",
    "        # for even image width use odd kernel width so that resulting img width is divisible by 2 during pooling\n",
    "        if (img_width % 2) == 0:\n",
    "            kernel_size = my_params[f'kernel_conv{i}_odd']\n",
    "        else:\n",
    "            kernel_size = my_params[f'kernel_conv{i}_even']\n",
    "        layers.append(nn.Conv2d(in_ch, out_ch, kernel_size))\n",
    "\n",
    "        layers.append(nn.ReLU())  # activation function\n",
    "        layers.append(nn.MaxPool2d(2,2))  # pooling layer\n",
    "\n",
    "        in_ch = out_ch  # no. of input channels for next layer = no. of output channels from this layer\n",
    "        img_width = int((img_width-(kernel_size-1))/2)\n",
    "\n",
    "    layers.append(nn.Flatten(start_dim=1))  # flatten all dimensions except batch\n",
    "\n",
    "    # Define Linear Layers\n",
    "    in_features = in_ch * img_width * img_width\n",
    "    for i in range(my_params['n_linear_layers']):\n",
    "        # linear layer\n",
    "        out_features = my_params[f'n_units_lin{i}']\n",
    "        layers.append(nn.Linear(in_features, out_features))\n",
    "\n",
    "        layers.append(nn.ReLU())  # activation function\n",
    "\n",
    "        #drop-out regularisation\n",
    "        p = my_params[f\"dropout_lin{i}\"]\n",
    "        layers.append(nn.Dropout(p))\n",
    "\n",
    "        in_features = out_features  # no. of inputs for next layer = no. of outputs of this layer\n",
    "\n",
    "    layers.append(nn.Linear(in_features, 10))  # output layer\n",
    "\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9368d3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_val_dataloader(training_dataset, my_batchsize, my_seed = None):\n",
    "    \"\"\"\n",
    "    Function to split training data into training and validation subsets and format as dataloaders\n",
    "    Model performance on validation set will be used for hyperparameter tuning.\n",
    "\n",
    "    :param training_dataset: full set of training data, in pytorch Dataset format\n",
    "    :param my_batchsize: batch size for pytorch DataLoader\n",
    "    :param my_seed: optional seed to be used for train test split random state\n",
    "\n",
    "    :return: tuple of pytorch DataLoaders - train_dataloader, val_dataloader\n",
    "    \"\"\"\n",
    "\n",
    "    # separate into training & validation datasets\n",
    "    total_len = len(training_dataset.data)\n",
    "    train, val = torch.utils.data.random_split(dataset=training_dataset, lengths=[int(0.8*total_len), int(0.2*total_len)])\n",
    "\n",
    "    #format as pytorch dataloader\n",
    "    train_dataloader = DataLoader(train, batch_size=my_batchsize, shuffle=True)\n",
    "    val_dataloader = DataLoader(val, batch_size=my_batchsize)\n",
    "\n",
    "    return train_dataloader, val_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9fd3ad75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_correct(predictions, y):\n",
    "    \"\"\"\n",
    "    Counts number of correct predictions in a batch\n",
    "    :param predictions: 1D tensor with predictions\n",
    "    :param y: 1D tensor with true classes\n",
    "    :return: number of correct predictions (pred==y)\n",
    "    \"\"\"\n",
    "    predictions = predictions.numpy()\n",
    "    y = y.numpy()\n",
    "\n",
    "    n_correct = (predictions == y).sum()\n",
    "\n",
    "    return n_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c7e9d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial, my_optimizer):\n",
    "    \"\"\"\n",
    "    Objective for Optuna to optimise\n",
    "    :param trial: Optuna trial object\n",
    "    :param optimizer_name: optimizer to use\n",
    "                            - SGD: SGD without momentum\n",
    "                            - SGD_classical: SGD with classical momentum\n",
    "                            - SGD_nesterov: SGD with nesterov momentum\n",
    "                            - Adam\n",
    "    :return: accuracy - fraction of correctly labelled validation points. This is what Optuna seeks to maximise\n",
    "    \"\"\"\n",
    "\n",
    "    #set parameters\n",
    "    my_params = set_parameters(trial, my_optimizer)\n",
    "\n",
    "    # Instantiate model\n",
    "    model = define_model(my_params)\n",
    "\n",
    "    # Instantiate optimizer\n",
    "    lr = my_params['lr']\n",
    "    if my_optimizer == 'SGD':\n",
    "        optimizer = getattr(optim, \"SGD\")(model.parameters(), lr=lr)\n",
    "    elif my_optimizer == 'SGD_classical':\n",
    "        momentum = my_params['momentum']\n",
    "        optimizer = getattr(optim, \"SGD\")(model.parameters(), lr=lr, momentum=momentum)\n",
    "    elif my_optimizer == 'SGD_nesterov':\n",
    "        momentum = my_params['momentum']\n",
    "        optimizer = getattr(optim, \"SGD\")(model.parameters(), lr=lr, momentum=momentum,\n",
    "                                                   nesterov=True)\n",
    "    elif my_optimizer == 'Adam':\n",
    "        beta1 = my_params['beta1']\n",
    "        beta2 = my_params['beta2']\n",
    "        optimizer = getattr(optim, \"Adam\")(model.parameters(), lr=lr, betas=(beta1, beta2))\n",
    "    else:\n",
    "        raise ValueError(\"optimizer_name must be 'SGD' / 'SGD_classical' / 'SGD_nesterov' / 'Adam'\")\n",
    "\n",
    "    # get data\n",
    "    train_dataloader, val_dataloader = get_train_val_dataloader(training_dataset=trainset,\n",
    "                                                                          my_batchsize=my_params['batch_size'])\n",
    "\n",
    "    # train model\n",
    "    for epoch in range(my_params['n_epochs']):\n",
    "\n",
    "        #train\n",
    "        model.train()\n",
    "        for batch, (X, y) in enumerate(train_dataloader):\n",
    "            # X and y are tensors. X.size() = (batch_size,n_features), y.size()=(batch_size,)\n",
    "            # set datatype for compatibility with nn.\n",
    "            X = X.float()\n",
    "            y = y.long()\n",
    "\n",
    "            # calculate model output and resulting loss\n",
    "            model_output = model(X)  # tensor. size=(batch_size x n_classes)\n",
    "            loss_fn = nn.CrossEntropyLoss() # instantiate loss function\n",
    "            loss = loss_fn(model_output, y)\n",
    "\n",
    "            # Backpropagation to update model weights\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # validate. We do this at each epoch to facilitate pruning:\n",
    "        # i.e. early termination of trials which are clearly not going to be optimum\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for batch, (X, y) in enumerate(val_dataloader):\n",
    "                X = X.float()\n",
    "                y = y.long()\n",
    "\n",
    "                # calculate model output and total number of correct predictions for this batch\n",
    "                model_output = model(X)\n",
    "                pred = torch.argmax(model_output, dim=1)  # prediction = class with highest output value\n",
    "                correct += count_correct(pred, y)\n",
    "\n",
    "        accuracy = correct / len(val_dataloader.dataset)\n",
    "\n",
    "        # report accuracy to allow Optuna to decide whether to prune this trial\n",
    "        trial.report(accuracy, epoch)\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    return accuracy  # return final validation accuracy after all epochs (unless pruned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e4bbf3",
   "metadata": {},
   "source": [
    "Optimisation algorithm is selected manually from:\n",
    "- \"SGD\": SGD without momentum\n",
    "- \"SGD_classical\": SGD with classical momentum\n",
    "- \"SGD_nesterov\": SGD with nesterov momentum\n",
    "- \"Adam\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6607c65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select optimisation algorithm (SGD/ SGD_classical, SGD_nesterov, Adam): SGD_nesterov\n"
     ]
    }
   ],
   "source": [
    "my_optim = input(\"Select optimisation algorithm (SGD/ SGD_classical, SGD_nesterov, Adam): \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c480d3c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-12-06 14:35:41,723]\u001b[0m A new study created in memory with name: no-name-bab891ab-1b12-432b-a62c-103e9253f286\u001b[0m\n",
      "\u001b[32m[I 2022-12-06 14:38:23,692]\u001b[0m Trial 0 finished with value: 0.3281 and parameters: {'n_conv_layers': 2, 'out_ch_conv0': 9, 'kernel_conv0_even': 6, 'kernel_conv0_odd': 7, 'out_ch_conv1': 24, 'kernel_conv1_even': 4, 'kernel_conv1_odd': 3, 'n_linear_layers': 3, 'n_units_lin0': 105, 'dropout_lin0': 0.7532499858263603, 'n_units_lin1': 104, 'dropout_lin1': 0.11838041596539997, 'n_units_lin2': 53, 'dropout_lin2': 0.7335057596935689, 'lr': 0.003954673847025026, 'batch_size': 10, 'n_epochs': 5, 'optimizer': 'SGD_nesterov', 'momentum': 0.7886969989376063}. Best is trial 0 with value: 0.3281.\u001b[0m\n",
      "\u001b[32m[I 2022-12-06 14:40:49,256]\u001b[0m Trial 1 finished with value: 0.6201 and parameters: {'n_conv_layers': 2, 'out_ch_conv0': 13, 'kernel_conv0_even': 2, 'kernel_conv0_odd': 7, 'out_ch_conv1': 34, 'kernel_conv1_even': 6, 'kernel_conv1_odd': 7, 'n_linear_layers': 1, 'n_units_lin0': 82, 'dropout_lin0': 0.10793984035358264, 'lr': 0.0036568160203629824, 'batch_size': 10, 'n_epochs': 5, 'optimizer': 'SGD_nesterov', 'momentum': 0.6700870992692292}. Best is trial 1 with value: 0.6201.\u001b[0m\n",
      "\u001b[32m[I 2022-12-06 14:43:17,829]\u001b[0m Trial 2 finished with value: 0.4592 and parameters: {'n_conv_layers': 2, 'out_ch_conv0': 5, 'kernel_conv0_even': 4, 'kernel_conv0_odd': 5, 'out_ch_conv1': 15, 'kernel_conv1_even': 6, 'kernel_conv1_odd': 5, 'n_linear_layers': 3, 'n_units_lin0': 40, 'dropout_lin0': 0.1219656342681728, 'n_units_lin1': 196, 'dropout_lin1': 0.5723408239114963, 'n_units_lin2': 99, 'dropout_lin2': 0.6144290803223919, 'lr': 0.0012021563399702442, 'batch_size': 10, 'n_epochs': 5, 'optimizer': 'SGD_nesterov', 'momentum': 0.8142521535238443}. Best is trial 1 with value: 0.6201.\u001b[0m\n",
      "\u001b[32m[I 2022-12-06 14:49:51,598]\u001b[0m Trial 3 finished with value: 0.1014 and parameters: {'n_conv_layers': 2, 'out_ch_conv0': 44, 'kernel_conv0_even': 4, 'kernel_conv0_odd': 5, 'out_ch_conv1': 14, 'kernel_conv1_even': 2, 'kernel_conv1_odd': 3, 'n_linear_layers': 3, 'n_units_lin0': 156, 'dropout_lin0': 0.4961314469488345, 'n_units_lin1': 151, 'dropout_lin1': 0.841806560598452, 'n_units_lin2': 83, 'dropout_lin2': 0.7739665333321928, 'lr': 0.03251459064529621, 'batch_size': 10, 'n_epochs': 5, 'optimizer': 'SGD_nesterov', 'momentum': 0.7279847106103253}. Best is trial 1 with value: 0.6201.\u001b[0m\n",
      "\u001b[32m[I 2022-12-06 14:52:59,251]\u001b[0m Trial 4 finished with value: 0.3132 and parameters: {'n_conv_layers': 2, 'out_ch_conv0': 31, 'kernel_conv0_even': 6, 'kernel_conv0_odd': 7, 'out_ch_conv1': 13, 'kernel_conv1_even': 4, 'kernel_conv1_odd': 5, 'n_linear_layers': 2, 'n_units_lin0': 106, 'dropout_lin0': 0.7235144904614516, 'n_units_lin1': 161, 'dropout_lin1': 0.15677739189396478, 'lr': 0.02071066450951882, 'batch_size': 10, 'n_epochs': 5, 'optimizer': 'SGD_nesterov', 'momentum': 0.6074604228845565}. Best is trial 1 with value: 0.6201.\u001b[0m\n",
      "\u001b[32m[I 2022-12-06 14:53:38,662]\u001b[0m Trial 5 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-12-06 14:54:20,363]\u001b[0m Trial 6 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-12-06 14:54:53,670]\u001b[0m Trial 7 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-12-06 14:55:24,299]\u001b[0m Trial 8 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-12-06 14:56:21,823]\u001b[0m Trial 9 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-12-06 14:57:24,771]\u001b[0m Trial 10 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-12-06 14:59:23,743]\u001b[0m Trial 11 finished with value: 0.5872 and parameters: {'n_conv_layers': 2, 'out_ch_conv0': 3, 'kernel_conv0_even': 2, 'kernel_conv0_odd': 3, 'out_ch_conv1': 24, 'kernel_conv1_even': 6, 'kernel_conv1_odd': 7, 'n_linear_layers': 1, 'n_units_lin0': 65, 'dropout_lin0': 0.11665215873452169, 'lr': 0.0022549358478816746, 'batch_size': 10, 'n_epochs': 5, 'optimizer': 'SGD_nesterov', 'momentum': 0.8503689399170357}. Best is trial 1 with value: 0.6201.\u001b[0m\n",
      "\u001b[32m[I 2022-12-06 15:01:35,519]\u001b[0m Trial 12 finished with value: 0.5554 and parameters: {'n_conv_layers': 2, 'out_ch_conv0': 2, 'kernel_conv0_even': 2, 'kernel_conv0_odd': 3, 'out_ch_conv1': 33, 'kernel_conv1_even': 6, 'kernel_conv1_odd': 7, 'n_linear_layers': 1, 'n_units_lin0': 74, 'dropout_lin0': 0.3075781597017038, 'lr': 0.002826960456302327, 'batch_size': 10, 'n_epochs': 5, 'optimizer': 'SGD_nesterov', 'momentum': 0.8531511025876122}. Best is trial 1 with value: 0.6201.\u001b[0m\n",
      "\u001b[32m[I 2022-12-06 15:04:07,301]\u001b[0m Trial 13 finished with value: 0.6684 and parameters: {'n_conv_layers': 2, 'out_ch_conv0': 15, 'kernel_conv0_even': 2, 'kernel_conv0_odd': 3, 'out_ch_conv1': 26, 'kernel_conv1_even': 6, 'kernel_conv1_odd': 7, 'n_linear_layers': 1, 'n_units_lin0': 132, 'dropout_lin0': 0.11956632109171272, 'lr': 0.007977862579619643, 'batch_size': 10, 'n_epochs': 5, 'optimizer': 'SGD_nesterov', 'momentum': 0.6675589569223452}. Best is trial 13 with value: 0.6684.\u001b[0m\n",
      "\u001b[32m[I 2022-12-06 15:04:53,678]\u001b[0m Trial 14 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-12-06 15:07:44,882]\u001b[0m Trial 15 finished with value: 0.6362 and parameters: {'n_conv_layers': 2, 'out_ch_conv0': 15, 'kernel_conv0_even': 2, 'kernel_conv0_odd': 3, 'out_ch_conv1': 41, 'kernel_conv1_even': 6, 'kernel_conv1_odd': 7, 'n_linear_layers': 1, 'n_units_lin0': 133, 'dropout_lin0': 0.22959689813814113, 'lr': 0.00922390025915152, 'batch_size': 10, 'n_epochs': 5, 'optimizer': 'SGD_nesterov', 'momentum': 0.6716695222831062}. Best is trial 13 with value: 0.6684.\u001b[0m\n",
      "\u001b[32m[I 2022-12-06 15:12:53,780]\u001b[0m Trial 16 finished with value: 0.6875 and parameters: {'n_conv_layers': 2, 'out_ch_conv0': 36, 'kernel_conv0_even': 2, 'kernel_conv0_odd': 3, 'out_ch_conv1': 42, 'kernel_conv1_even': 6, 'kernel_conv1_odd': 7, 'n_linear_layers': 1, 'n_units_lin0': 134, 'dropout_lin0': 0.24621121460085407, 'lr': 0.008053996454820373, 'batch_size': 10, 'n_epochs': 5, 'optimizer': 'SGD_nesterov', 'momentum': 0.6483740079018778}. Best is trial 16 with value: 0.6875.\u001b[0m\n",
      "\u001b[32m[I 2022-12-06 15:13:59,267]\u001b[0m Trial 17 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-12-06 15:15:00,730]\u001b[0m Trial 18 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-12-06 15:15:40,522]\u001b[0m Trial 19 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-12-06 15:16:19,319]\u001b[0m Trial 20 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-12-06 15:20:49,049]\u001b[0m Trial 21 finished with value: 0.684 and parameters: {'n_conv_layers': 2, 'out_ch_conv0': 20, 'kernel_conv0_even': 2, 'kernel_conv0_odd': 3, 'out_ch_conv1': 41, 'kernel_conv1_even': 6, 'kernel_conv1_odd': 7, 'n_linear_layers': 1, 'n_units_lin0': 128, 'dropout_lin0': 0.21871520304962877, 'lr': 0.006063826220627602, 'batch_size': 10, 'n_epochs': 5, 'optimizer': 'SGD_nesterov', 'momentum': 0.6680521875784755}. Best is trial 16 with value: 0.6875.\u001b[0m\n",
      "\u001b[32m[I 2022-12-06 15:23:57,084]\u001b[0m Trial 22 finished with value: 0.6389 and parameters: {'n_conv_layers': 2, 'out_ch_conv0': 20, 'kernel_conv0_even': 2, 'kernel_conv0_odd': 3, 'out_ch_conv1': 29, 'kernel_conv1_even': 6, 'kernel_conv1_odd': 7, 'n_linear_layers': 1, 'n_units_lin0': 118, 'dropout_lin0': 0.22681571480337964, 'lr': 0.00948279067518043, 'batch_size': 10, 'n_epochs': 5, 'optimizer': 'SGD_nesterov', 'momentum': 0.6516489578846604}. Best is trial 16 with value: 0.6875.\u001b[0m\n",
      "\u001b[32m[I 2022-12-06 15:24:55,571]\u001b[0m Trial 23 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-12-06 15:25:52,506]\u001b[0m Trial 24 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-12-06 15:26:27,985]\u001b[0m Trial 25 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-12-06 15:29:35,286]\u001b[0m Trial 26 finished with value: 0.6304 and parameters: {'n_conv_layers': 2, 'out_ch_conv0': 21, 'kernel_conv0_even': 2, 'kernel_conv0_odd': 3, 'out_ch_conv1': 39, 'kernel_conv1_even': 6, 'kernel_conv1_odd': 7, 'n_linear_layers': 1, 'n_units_lin0': 118, 'dropout_lin0': 0.16869673754130968, 'lr': 0.013543339066527119, 'batch_size': 10, 'n_epochs': 5, 'optimizer': 'SGD_nesterov', 'momentum': 0.706602476900046}. Best is trial 16 with value: 0.6875.\u001b[0m\n",
      "\u001b[32m[I 2022-12-06 15:30:35,602]\u001b[0m Trial 27 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-12-06 15:37:37,756]\u001b[0m Trial 28 finished with value: 0.6542 and parameters: {'n_conv_layers': 2, 'out_ch_conv0': 50, 'kernel_conv0_even': 2, 'kernel_conv0_odd': 3, 'out_ch_conv1': 18, 'kernel_conv1_even': 6, 'kernel_conv1_odd': 3, 'n_linear_layers': 1, 'n_units_lin0': 97, 'dropout_lin0': 0.17107421445842347, 'lr': 0.005664853108883316, 'batch_size': 10, 'n_epochs': 5, 'optimizer': 'SGD_nesterov', 'momentum': 0.6575476925380245}. Best is trial 16 with value: 0.6875.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-12-06 15:38:11,053]\u001b[0m Trial 29 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-12-06 15:38:54,164]\u001b[0m Trial 30 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-12-06 15:43:51,232]\u001b[0m Trial 31 finished with value: 0.6586 and parameters: {'n_conv_layers': 2, 'out_ch_conv0': 49, 'kernel_conv0_even': 2, 'kernel_conv0_odd': 3, 'out_ch_conv1': 20, 'kernel_conv1_even': 6, 'kernel_conv1_odd': 3, 'n_linear_layers': 1, 'n_units_lin0': 93, 'dropout_lin0': 0.18427834760558845, 'lr': 0.005952288879299075, 'batch_size': 10, 'n_epochs': 5, 'optimizer': 'SGD_nesterov', 'momentum': 0.6490634484539481}. Best is trial 16 with value: 0.6875.\u001b[0m\n",
      "\u001b[32m[I 2022-12-06 15:48:23,962]\u001b[0m Trial 32 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-12-06 15:53:08,411]\u001b[0m Trial 33 finished with value: 0.6824 and parameters: {'n_conv_layers': 2, 'out_ch_conv0': 44, 'kernel_conv0_even': 2, 'kernel_conv0_odd': 3, 'out_ch_conv1': 31, 'kernel_conv1_even': 6, 'kernel_conv1_odd': 3, 'n_linear_layers': 1, 'n_units_lin0': 97, 'dropout_lin0': 0.198053644754176, 'lr': 0.004606138715516063, 'batch_size': 10, 'n_epochs': 5, 'optimizer': 'SGD_nesterov', 'momentum': 0.6686019415812707}. Best is trial 16 with value: 0.6875.\u001b[0m\n",
      "\u001b[32m[I 2022-12-06 15:54:07,580]\u001b[0m Trial 34 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-12-06 15:55:00,122]\u001b[0m Trial 35 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-12-06 15:55:44,181]\u001b[0m Trial 36 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-12-06 15:56:47,021]\u001b[0m Trial 37 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-12-06 15:59:37,776]\u001b[0m Trial 38 finished with value: 0.6373 and parameters: {'n_conv_layers': 2, 'out_ch_conv0': 23, 'kernel_conv0_even': 6, 'kernel_conv0_odd': 7, 'out_ch_conv1': 32, 'kernel_conv1_even': 2, 'kernel_conv1_odd': 3, 'n_linear_layers': 1, 'n_units_lin0': 82, 'dropout_lin0': 0.10318415757882536, 'lr': 0.011581799871580752, 'batch_size': 10, 'n_epochs': 5, 'optimizer': 'SGD_nesterov', 'momentum': 0.6191323399109188}. Best is trial 16 with value: 0.6875.\u001b[0m\n",
      "\u001b[32m[I 2022-12-06 16:00:37,031]\u001b[0m Trial 39 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-12-06 16:01:26,560]\u001b[0m Trial 40 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-12-06 16:06:35,208]\u001b[0m Trial 41 finished with value: 0.6556 and parameters: {'n_conv_layers': 2, 'out_ch_conv0': 50, 'kernel_conv0_even': 2, 'kernel_conv0_odd': 3, 'out_ch_conv1': 17, 'kernel_conv1_even': 6, 'kernel_conv1_odd': 3, 'n_linear_layers': 1, 'n_units_lin0': 94, 'dropout_lin0': 0.19599884326595798, 'lr': 0.005741692545427952, 'batch_size': 10, 'n_epochs': 5, 'optimizer': 'SGD_nesterov', 'momentum': 0.6538540698661197}. Best is trial 16 with value: 0.6875.\u001b[0m\n",
      "\u001b[32m[I 2022-12-06 16:07:28,559]\u001b[0m Trial 42 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-12-06 16:08:23,399]\u001b[0m Trial 43 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-12-06 16:09:19,129]\u001b[0m Trial 44 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-12-06 16:10:00,151]\u001b[0m Trial 45 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-12-06 16:10:29,790]\u001b[0m Trial 46 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-12-06 16:11:33,726]\u001b[0m Trial 47 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-12-06 16:12:04,571]\u001b[0m Trial 48 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-12-06 16:13:17,845]\u001b[0m Trial 49 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-12-06 16:14:02,328]\u001b[0m Trial 50 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-12-06 16:14:55,479]\u001b[0m Trial 51 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-12-06 16:20:04,471]\u001b[0m Trial 52 finished with value: 0.6441 and parameters: {'n_conv_layers': 2, 'out_ch_conv0': 50, 'kernel_conv0_even': 2, 'kernel_conv0_odd': 3, 'out_ch_conv1': 18, 'kernel_conv1_even': 6, 'kernel_conv1_odd': 3, 'n_linear_layers': 1, 'n_units_lin0': 100, 'dropout_lin0': 0.138024283133691, 'lr': 0.005946220998342888, 'batch_size': 10, 'n_epochs': 5, 'optimizer': 'SGD_nesterov', 'momentum': 0.6466254327643671}. Best is trial 16 with value: 0.6875.\u001b[0m\n",
      "\u001b[32m[I 2022-12-06 16:20:59,815]\u001b[0m Trial 53 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-12-06 16:27:02,061]\u001b[0m Trial 54 finished with value: 0.6746 and parameters: {'n_conv_layers': 2, 'out_ch_conv0': 49, 'kernel_conv0_even': 2, 'kernel_conv0_odd': 3, 'out_ch_conv1': 34, 'kernel_conv1_even': 6, 'kernel_conv1_odd': 3, 'n_linear_layers': 1, 'n_units_lin0': 76, 'dropout_lin0': 0.29595109358611216, 'lr': 0.007924933820908283, 'batch_size': 10, 'n_epochs': 5, 'optimizer': 'SGD_nesterov', 'momentum': 0.7147863755353595}. Best is trial 16 with value: 0.6875.\u001b[0m\n",
      "\u001b[32m[I 2022-12-06 16:27:59,204]\u001b[0m Trial 55 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-12-06 16:28:33,156]\u001b[0m Trial 56 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-12-06 16:29:44,739]\u001b[0m Trial 57 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-12-06 16:31:58,996]\u001b[0m Trial 58 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-12-06 16:32:33,386]\u001b[0m Trial 59 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-12-06 16:36:58,216]\u001b[0m Trial 60 finished with value: 0.6565 and parameters: {'n_conv_layers': 2, 'out_ch_conv0': 35, 'kernel_conv0_even': 6, 'kernel_conv0_odd': 3, 'out_ch_conv1': 40, 'kernel_conv1_even': 4, 'kernel_conv1_odd': 7, 'n_linear_layers': 1, 'n_units_lin0': 112, 'dropout_lin0': 0.16021400218580792, 'lr': 0.016002618024997656, 'batch_size': 10, 'n_epochs': 5, 'optimizer': 'SGD_nesterov', 'momentum': 0.6203665285813533}. Best is trial 16 with value: 0.6875.\u001b[0m\n",
      "\u001b[32m[I 2022-12-06 16:41:23,208]\u001b[0m Trial 61 finished with value: 0.64 and parameters: {'n_conv_layers': 2, 'out_ch_conv0': 34, 'kernel_conv0_even': 6, 'kernel_conv0_odd': 3, 'out_ch_conv1': 40, 'kernel_conv1_even': 4, 'kernel_conv1_odd': 7, 'n_linear_layers': 1, 'n_units_lin0': 108, 'dropout_lin0': 0.16466140066683263, 'lr': 0.016631615765849758, 'batch_size': 10, 'n_epochs': 5, 'optimizer': 'SGD_nesterov', 'momentum': 0.6209527700170229}. Best is trial 16 with value: 0.6875.\u001b[0m\n",
      "\u001b[32m[I 2022-12-06 16:42:12,212]\u001b[0m Trial 62 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-12-06 16:45:02,108]\u001b[0m Trial 63 finished with value: 0.6569 and parameters: {'n_conv_layers': 2, 'out_ch_conv0': 12, 'kernel_conv0_even': 6, 'kernel_conv0_odd': 3, 'out_ch_conv1': 37, 'kernel_conv1_even': 4, 'kernel_conv1_odd': 7, 'n_linear_layers': 1, 'n_units_lin0': 133, 'dropout_lin0': 0.23253688420182902, 'lr': 0.009131892567515778, 'batch_size': 10, 'n_epochs': 5, 'optimizer': 'SGD_nesterov', 'momentum': 0.6643190860177836}. Best is trial 16 with value: 0.6875.\u001b[0m\n",
      "\u001b[32m[I 2022-12-06 16:45:36,289]\u001b[0m Trial 64 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-12-06 16:48:58,509]\u001b[0m Trial 65 finished with value: 0.6852 and parameters: {'n_conv_layers': 2, 'out_ch_conv0': 19, 'kernel_conv0_even': 6, 'kernel_conv0_odd': 3, 'out_ch_conv1': 34, 'kernel_conv1_even': 4, 'kernel_conv1_odd': 7, 'n_linear_layers': 1, 'n_units_lin0': 140, 'dropout_lin0': 0.2831050824863741, 'lr': 0.010014236396469157, 'batch_size': 10, 'n_epochs': 5, 'optimizer': 'SGD_nesterov', 'momentum': 0.6643285389170825}. Best is trial 16 with value: 0.6875.\u001b[0m\n",
      "\u001b[32m[I 2022-12-06 16:49:35,847]\u001b[0m Trial 66 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-12-06 16:50:16,331]\u001b[0m Trial 67 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-12-06 16:50:56,850]\u001b[0m Trial 68 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-12-06 16:51:51,763]\u001b[0m Trial 69 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-12-06 16:54:11,949]\u001b[0m Trial 70 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-12-06 16:56:53,075]\u001b[0m Trial 71 finished with value: 0.6442 and parameters: {'n_conv_layers': 2, 'out_ch_conv0': 11, 'kernel_conv0_even': 6, 'kernel_conv0_odd': 3, 'out_ch_conv1': 38, 'kernel_conv1_even': 4, 'kernel_conv1_odd': 7, 'n_linear_layers': 1, 'n_units_lin0': 136, 'dropout_lin0': 0.22809380614280442, 'lr': 0.010401474809404369, 'batch_size': 10, 'n_epochs': 5, 'optimizer': 'SGD_nesterov', 'momentum': 0.6584377955512726}. Best is trial 16 with value: 0.6875.\u001b[0m\n",
      "\u001b[32m[I 2022-12-06 16:59:42,726]\u001b[0m Trial 72 finished with value: 0.6769 and parameters: {'n_conv_layers': 2, 'out_ch_conv0': 13, 'kernel_conv0_even': 6, 'kernel_conv0_odd': 3, 'out_ch_conv1': 35, 'kernel_conv1_even': 4, 'kernel_conv1_odd': 7, 'n_linear_layers': 1, 'n_units_lin0': 124, 'dropout_lin0': 0.17668038609932787, 'lr': 0.00977465506049154, 'batch_size': 10, 'n_epochs': 5, 'optimizer': 'SGD_nesterov', 'momentum': 0.663898269095445}. Best is trial 16 with value: 0.6875.\u001b[0m\n",
      "\u001b[32m[I 2022-12-06 17:00:15,524]\u001b[0m Trial 73 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-12-06 17:03:28,043]\u001b[0m Trial 74 finished with value: 0.646 and parameters: {'n_conv_layers': 2, 'out_ch_conv0': 22, 'kernel_conv0_even': 6, 'kernel_conv0_odd': 3, 'out_ch_conv1': 34, 'kernel_conv1_even': 4, 'kernel_conv1_odd': 7, 'n_linear_layers': 1, 'n_units_lin0': 123, 'dropout_lin0': 0.3269480475487539, 'lr': 0.012941335952482874, 'batch_size': 10, 'n_epochs': 5, 'optimizer': 'SGD_nesterov', 'momentum': 0.6848014794616223}. Best is trial 16 with value: 0.6875.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-12-06 17:04:02,365]\u001b[0m Trial 75 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-12-06 17:05:02,940]\u001b[0m Trial 76 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-12-06 17:10:35,988]\u001b[0m Trial 77 finished with value: 0.6904 and parameters: {'n_conv_layers': 2, 'out_ch_conv0': 45, 'kernel_conv0_even': 2, 'kernel_conv0_odd': 3, 'out_ch_conv1': 44, 'kernel_conv1_even': 6, 'kernel_conv1_odd': 3, 'n_linear_layers': 1, 'n_units_lin0': 154, 'dropout_lin0': 0.17705477709707426, 'lr': 0.00427669587129408, 'batch_size': 10, 'n_epochs': 5, 'optimizer': 'SGD_nesterov', 'momentum': 0.6999135583225274}. Best is trial 77 with value: 0.6904.\u001b[0m\n",
      "\u001b[32m[I 2022-12-06 17:11:38,289]\u001b[0m Trial 78 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-12-06 17:12:33,002]\u001b[0m Trial 79 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-12-06 17:13:12,708]\u001b[0m Trial 80 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-12-06 17:18:46,577]\u001b[0m Trial 81 finished with value: 0.7056 and parameters: {'n_conv_layers': 2, 'out_ch_conv0': 46, 'kernel_conv0_even': 2, 'kernel_conv0_odd': 3, 'out_ch_conv1': 50, 'kernel_conv1_even': 6, 'kernel_conv1_odd': 3, 'n_linear_layers': 1, 'n_units_lin0': 122, 'dropout_lin0': 0.18699223741707605, 'lr': 0.006360806468362518, 'batch_size': 10, 'n_epochs': 5, 'optimizer': 'SGD_nesterov', 'momentum': 0.6606857381635803}. Best is trial 81 with value: 0.7056.\u001b[0m\n",
      "\u001b[32m[I 2022-12-06 17:24:11,206]\u001b[0m Trial 82 finished with value: 0.6957 and parameters: {'n_conv_layers': 2, 'out_ch_conv0': 45, 'kernel_conv0_even': 2, 'kernel_conv0_odd': 3, 'out_ch_conv1': 49, 'kernel_conv1_even': 6, 'kernel_conv1_odd': 3, 'n_linear_layers': 1, 'n_units_lin0': 125, 'dropout_lin0': 0.15723988188483073, 'lr': 0.006888090736772421, 'batch_size': 10, 'n_epochs': 5, 'optimizer': 'SGD_nesterov', 'momentum': 0.6642343945179585}. Best is trial 81 with value: 0.7056.\u001b[0m\n",
      "\u001b[32m[I 2022-12-06 17:25:15,563]\u001b[0m Trial 83 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-12-06 17:30:26,767]\u001b[0m Trial 84 finished with value: 0.7033 and parameters: {'n_conv_layers': 2, 'out_ch_conv0': 46, 'kernel_conv0_even': 2, 'kernel_conv0_odd': 3, 'out_ch_conv1': 48, 'kernel_conv1_even': 6, 'kernel_conv1_odd': 3, 'n_linear_layers': 1, 'n_units_lin0': 130, 'dropout_lin0': 0.1789305605398462, 'lr': 0.004840736713543741, 'batch_size': 10, 'n_epochs': 5, 'optimizer': 'SGD_nesterov', 'momentum': 0.6749895815394398}. Best is trial 81 with value: 0.7056.\u001b[0m\n",
      "\u001b[32m[I 2022-12-06 17:35:31,978]\u001b[0m Trial 85 finished with value: 0.7029 and parameters: {'n_conv_layers': 2, 'out_ch_conv0': 46, 'kernel_conv0_even': 2, 'kernel_conv0_odd': 3, 'out_ch_conv1': 48, 'kernel_conv1_even': 6, 'kernel_conv1_odd': 3, 'n_linear_layers': 1, 'n_units_lin0': 130, 'dropout_lin0': 0.18444748544749834, 'lr': 0.006712443934036949, 'batch_size': 10, 'n_epochs': 5, 'optimizer': 'SGD_nesterov', 'momentum': 0.6793441085912298}. Best is trial 81 with value: 0.7056.\u001b[0m\n",
      "\u001b[32m[I 2022-12-06 17:36:31,758]\u001b[0m Trial 86 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-12-06 17:37:36,599]\u001b[0m Trial 87 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-12-06 17:38:25,290]\u001b[0m Trial 88 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-12-06 17:39:29,975]\u001b[0m Trial 89 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-12-06 17:40:31,613]\u001b[0m Trial 90 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-12-06 17:45:33,230]\u001b[0m Trial 91 finished with value: 0.6928 and parameters: {'n_conv_layers': 2, 'out_ch_conv0': 47, 'kernel_conv0_even': 2, 'kernel_conv0_odd': 3, 'out_ch_conv1': 48, 'kernel_conv1_even': 6, 'kernel_conv1_odd': 3, 'n_linear_layers': 1, 'n_units_lin0': 127, 'dropout_lin0': 0.18325571726844367, 'lr': 0.010187525005020103, 'batch_size': 10, 'n_epochs': 5, 'optimizer': 'SGD_nesterov', 'momentum': 0.6814954873323935}. Best is trial 81 with value: 0.7056.\u001b[0m\n",
      "\u001b[32m[I 2022-12-06 17:48:36,886]\u001b[0m Trial 92 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-12-06 17:53:26,623]\u001b[0m Trial 93 finished with value: 0.7052 and parameters: {'n_conv_layers': 2, 'out_ch_conv0': 44, 'kernel_conv0_even': 2, 'kernel_conv0_odd': 3, 'out_ch_conv1': 47, 'kernel_conv1_even': 6, 'kernel_conv1_odd': 3, 'n_linear_layers': 1, 'n_units_lin0': 127, 'dropout_lin0': 0.18745767083440748, 'lr': 0.006641669659607287, 'batch_size': 10, 'n_epochs': 5, 'optimizer': 'SGD_nesterov', 'momentum': 0.6283413419440914}. Best is trial 81 with value: 0.7056.\u001b[0m\n",
      "\u001b[32m[I 2022-12-06 17:55:23,515]\u001b[0m Trial 94 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-12-06 18:00:21,493]\u001b[0m Trial 95 finished with value: 0.6707 and parameters: {'n_conv_layers': 2, 'out_ch_conv0': 47, 'kernel_conv0_even': 2, 'kernel_conv0_odd': 3, 'out_ch_conv1': 45, 'kernel_conv1_even': 6, 'kernel_conv1_odd': 3, 'n_linear_layers': 1, 'n_units_lin0': 139, 'dropout_lin0': 0.1570156002991684, 'lr': 0.012764136319628031, 'batch_size': 10, 'n_epochs': 5, 'optimizer': 'SGD_nesterov', 'momentum': 0.6378316218202031}. Best is trial 81 with value: 0.7056.\u001b[0m\n",
      "\u001b[32m[I 2022-12-06 18:02:17,451]\u001b[0m Trial 96 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-12-06 18:03:20,510]\u001b[0m Trial 97 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-12-06 18:08:11,535]\u001b[0m Trial 98 finished with value: 0.6689 and parameters: {'n_conv_layers': 2, 'out_ch_conv0': 42, 'kernel_conv0_even': 2, 'kernel_conv0_odd': 3, 'out_ch_conv1': 46, 'kernel_conv1_even': 6, 'kernel_conv1_odd': 3, 'n_linear_layers': 1, 'n_units_lin0': 120, 'dropout_lin0': 0.13305211639237566, 'lr': 0.004959383391285208, 'batch_size': 10, 'n_epochs': 5, 'optimizer': 'SGD_nesterov', 'momentum': 0.8804655772045269}. Best is trial 81 with value: 0.7056.\u001b[0m\n",
      "\u001b[32m[I 2022-12-06 18:09:02,881]\u001b[0m Trial 99 pruned. \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# instantiate optuna study\n",
    "study = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler())\n",
    "# Optimise hyperparameters will try {n_trials} param combinations or till {timeout} seconds is hit\n",
    "study.optimize(lambda trial: objective(trial, my_optim), n_trials=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14a7a135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Study statistics: \n",
      "  Number of finished trials:  100\n",
      "  Number of pruned trials:  64\n",
      "  Number of complete trials:  36\n",
      "Best trial:\n",
      "  Validation Accuracy:  0.7056\n",
      "  Params: \n",
      "    n_conv_layers: 2\n",
      "    out_ch_conv0: 46\n",
      "    kernel_conv0_even: 2\n",
      "    kernel_conv0_odd: 3\n",
      "    out_ch_conv1: 50\n",
      "    kernel_conv1_even: 6\n",
      "    kernel_conv1_odd: 3\n",
      "    n_linear_layers: 1\n",
      "    n_units_lin0: 122\n",
      "    dropout_lin0: 0.18699223741707605\n",
      "    lr: 0.006360806468362518\n",
      "    batch_size: 10\n",
      "    n_epochs: 5\n",
      "    optimizer: SGD_nesterov\n",
      "    momentum: 0.6606857381635803\n"
     ]
    }
   ],
   "source": [
    "#display study results\n",
    "pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
    "complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n",
    "\n",
    "print(\"Study statistics: \")\n",
    "print(\"  Number of finished trials: \", len(study.trials))\n",
    "print(\"  Number of pruned trials: \", len(pruned_trials))\n",
    "print(\"  Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "best_trial = study.best_trial\n",
    "\n",
    "print(\"  Validation Accuracy: \", best_trial.value)\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in best_trial.params.items():\n",
    "    print(f\"    {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4de503e",
   "metadata": {},
   "source": [
    "**Train final model using tuned hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "864bb8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_final_model(my_params):\n",
    "    \"\"\"\n",
    "    Train final model using tuned hyperparameters from best Optuna trial\n",
    "    :param my_params: dictionary of parameters from Optuna trial object that had best validation accuracy\n",
    "\n",
    "    :return: model\n",
    "    \"\"\"\n",
    "\n",
    "    # Instantiate model\n",
    "    model = define_model(my_params)\n",
    "\n",
    "    # Instantiate optimizer\n",
    "    my_optimizer = my_params['optimizer']\n",
    "    lr = my_params['lr']\n",
    "    if my_optimizer == 'SGD':\n",
    "        optimizer = getattr(optim, \"SGD\")(model.parameters(), lr=lr)\n",
    "    elif my_optimizer == 'SGD_classical':\n",
    "        momentum = my_params['momentum']\n",
    "        optimizer = getattr(optim, \"SGD\")(model.parameters(), lr=lr, momentum=momentum)\n",
    "    elif my_optimizer == 'SGD_nesterov':\n",
    "        momentum = my_params['momentum']\n",
    "        optimizer = getattr(optim, \"SGD\")(model.parameters(), lr=lr, momentum=momentum,\n",
    "                                          nesterov=True)\n",
    "    elif my_optimizer == 'Adam':\n",
    "        beta1 = my_params['beta1']\n",
    "        beta2 = my_params['beta2']\n",
    "        optimizer = getattr(optim, \"Adam\")(model.parameters(), lr=lr, betas=(beta1, beta2))\n",
    "    else:\n",
    "        raise ValueError(\"optimizer_name must be 'SGD' / 'SGD_classical' / 'SGD_nesterov' / 'Adam'\")\n",
    "\n",
    "    # get data\n",
    "    train_dataloader = DataLoader(dataset=trainset, batch_size=my_params['batch_size'])\n",
    "\n",
    "    # train model\n",
    "    for epoch in range(my_params['n_epochs']):\n",
    "        model.train()\n",
    "        for batch, (X, y) in enumerate(train_dataloader):\n",
    "            # set datatype for compatibility with nn.\n",
    "            X = X.float()\n",
    "            y = y.long()\n",
    "\n",
    "            # calculate model output and resulting loss\n",
    "            model_output = model(X)  # tensor. size=(batch_size x n_classes)\n",
    "            loss_fn = nn.CrossEntropyLoss()  # instantiate loss function\n",
    "            loss = loss_fn(model_output, y)\n",
    "\n",
    "            # Backpropagation to update model weights\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3acc2bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = best_trial.params\n",
    "final_model = train_final_model(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bf8b66a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Final Training Accuracy: 0.79506\n"
     ]
    }
   ],
   "source": [
    "# EVALUATE FINAL TRAINING ACCURACY\n",
    "def predict_and_evaluate(model, my_dataset):\n",
    "    \"\"\"\n",
    "    Function to run trained and tuned model on provided dataframe to obtain predictions and evaluate\n",
    "    accuracy\n",
    "\n",
    "    :param model: trained model\n",
    "    :param my_dataset: dataset including features and target/label\n",
    "\n",
    "    :return: accuracy\n",
    "    \"\"\"\n",
    "    my_dataloader = DataLoader(my_dataset, batch_size=10, shuffle=False)\n",
    "\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for batch, (X, y) in enumerate(my_dataloader):\n",
    "            X = X.float()\n",
    "            y = y.long()\n",
    "\n",
    "            # calculate model output and total number of correct predictions for this batch\n",
    "            model_output = model(X)\n",
    "            pred = torch.argmax(model_output, dim=1)  # prediction = class with highest output value\n",
    "            correct += count_correct(pred, y)\n",
    "\n",
    "    accuracy = correct / len(my_dataloader.dataset)\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "train_acc = predict_and_evaluate(final_model, trainset)\n",
    "print(f\"  Final Training Accuracy: {train_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccd8ae3",
   "metadata": {},
   "source": [
    "## <font color='blue'>Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f28c3c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "  Test Accuracy: 0.7116\n"
     ]
    }
   ],
   "source": [
    "testset = get_CIFAR(train = False)\n",
    "test_acc = predict_and_evaluate(final_model, testset)\n",
    "print(f\"  Test Accuracy: {test_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d15d6a",
   "metadata": {},
   "source": [
    "## <font color='blue'>Save Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cbcffdcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(final_model, 'cnn_' + best_params['optimizer'] + '.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4122a4",
   "metadata": {},
   "source": [
    "## <font color = 'blue'> Past results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6373590f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_seed=42\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cnn_SGD_nesterov</td>\n",
       "      <td>0.7116</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              model  accuracy\n",
       "0  cnn_SGD_nesterov    0.7116"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('my_seed=42')\n",
    "my_results = {'cnn_SGD_nesterov':0.7116}\n",
    "\n",
    "myresults_df = pd.DataFrame.from_dict(my_results, 'index').reset_index().rename({'index':'model',0:'accuracy'},axis=1)\n",
    "myresults_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f29adcb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
